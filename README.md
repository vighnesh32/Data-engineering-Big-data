# Big-Data
##### 1. HDFS and LFS commands - This file contains the basic hdfs and lfs commands.
##### 2. Hive code 1 - This code creates a Hive database called "hive_1" and creates tables within it to store department and employee data. It also demonstrates loading data from both local and HDFS locations and working with array and map data types. The code includes examples of selecting and manipulating data within these data types using Hive's built-in functions. Overall, the code showcases basic operations in Hive for creating and working with structured data.
##### 3. Hive code 2 - This script creates a table from a CSV file, makes a backup of it, retrieves the table's details, and then creates another table with the same data in the Parquet format, which is optimized for big data processing.
##### 4. Hive code 3 - This code showcases various Hive commands used for creating tables, loading data, and performing queries. It demonstrates the creation of tables with different SerDe formats, loading data from local files, and downloading jar files for SerDe libraries. It also covers changing the number of reducers, creating partitioned tables with static and dynamic partitioning, and inserting data into partitioned tables. In summary, this code provides an overview of several essential Hive concepts, including table creation, data loading, partitioning, and query execution.
##### 5. Hive code 4 - This code contains a series of HiveQL statements demonstrating different features of Apache Hive. It includes commands for creating tables and loading data, adding UDFs written in Python, bucketing and joining tables using different techniques such as reduce-side join, map-side join, bucket map join, and sorted merge bucket map join. The code also includes comments describing the purpose and outcome of each command.
##### 6. Kafka code 1 - This file provides an data pipeline example using Confluent Cloud, which involves creating a Kafka cluster, configuring API keys, creating a topic, obtaining Schema Registry details, writing the Python code, and running the code. The Python code defines a Car class to hold data, a JSON schema for the Car data, methods to serialize and deserialize the Car objects, a producer that sends Car data to a Kafka topic, and a consumer that reads Car data from the same Kafka topic. To use the code, you would need to replace the API keys, secret keys, endpoint URL, and other variables with the correct values for your Kafka cluster on Confluent Cloud.
